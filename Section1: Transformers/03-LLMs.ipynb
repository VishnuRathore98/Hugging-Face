{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509f92c8",
   "metadata": {},
   "source": [
    "# Understanding LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e812fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03598ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663c682",
   "metadata": {},
   "source": [
    "## Tokenizing Text\n",
    "\n",
    "### Why Tokenization?\n",
    "\n",
    "Tokenization transforms text into a format that models can comprehend. There are several methods for tokenizing text, each with its pros and cons:\n",
    "\n",
    "1. **Character-Based Tokenization**:\n",
    "   - **Method**: Splitting the text into individual characters and assigning each a unique numerical ID.\n",
    "   - **Pros**: Works well for languages like Chinese, where each character carries significant information.\n",
    "   - **Cons**: Creates a small vocabulary but requires many tokens to represent a string. This can affect performance and accuracy since individual characters carry minimal information.\n",
    "\n",
    "2. **Word-Based Tokenization**:\n",
    "   - **Method**: Splitting the text into individual words.\n",
    "   - **Pros**: Captures more meaning per token.\n",
    "   - **Cons**: Results in a large vocabulary with many unknown words (e.g., typos, slang) and different word forms (e.g., \"run\", \"runs\", \"running\").\n",
    "\n",
    "### Modern Tokenization Strategies\n",
    "\n",
    "Modern approaches balance character and word tokenization by splitting text into subwords. These methods effectively capture both the structure and meaning of the text while efficiently handling unknown words and different forms of the same word.\n",
    "\n",
    "- **Subword Tokenization**:\n",
    "  - **Method**: Frequently occurring words or subwords are assigned a single token, while complex words are split into multiple tokens, each representing a meaningful part of the word.\n",
    "  - **Example**: \"flabbergasted\" could be split into:\n",
    "              \n",
    "              tensor(781) \t:  fl\n",
    "              tensor(397) \t: ab\n",
    "              tensor(3900) \t: berg\n",
    "              tensor(8992) \t: asted\n",
    "\n",
    "Different models use different tokenizers, each with its unique strategy and vocabulary size. Let's see how the GPT-2 tokenizer handles a sentence.\n",
    "\n",
    "### Example with GPT-2 Tokenizer\n",
    "\n",
    "We'll use the GPT-2 tokenizer to tokenize the sentence shown below. This involves converting the text into tokens and then decoding those tokens back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff837966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[37534,  6197,   516,    11,   314,  1101,  1610,   397,  3900,  8992,\n",
      "             0]])\n",
      "tensor(37534) \t: Prep\n",
      "tensor(6197) \t: oster\n",
      "tensor(516) \t: ous\n",
      "tensor(11) \t: ,\n",
      "tensor(314) \t:  I\n",
      "tensor(1101) \t: 'm\n",
      "tensor(1610) \t:  Fl\n",
      "tensor(397) \t: ab\n",
      "tensor(3900) \t: berg\n",
      "tensor(8992) \t: asted\n",
      "tensor(0) \t: !\n"
     ]
    }
   ],
   "source": [
    "# Loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "# Getting the token ids\n",
    "input_ids = tokenizer(\"Preposterous, I'm Flabbergasted!\", return_tensors='pt').input_ids\n",
    "print(input_ids)\n",
    "# Decoding the tokens back into text\n",
    "for t in input_ids[0]:\n",
    "    print(t,'\\t:', tokenizer.decode(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58bbdc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40) \t: I\n",
      "tensor(14267) \t:  skip\n",
      "tensor(1973) \t:  across\n",
      "tensor(262) \t:  the\n"
     ]
    }
   ],
   "source": [
    "input_ids2 = tokenizer(\"I skip across the\", return_tensors=\"pt\").input_ids\n",
    "for t2 in input_ids2[0]:\n",
    "    print(t2, \"\\t:\", tokenizer.decode(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf751332",
   "metadata": {},
   "source": [
    "As shown, the tokenizer splits the input string into a series of tokens, each assigned a unique ID. Most words are represented by a single token, but longer words (or even shorter ones!) can be split into multiple tokens. Play around with this!\n",
    "\n",
    "### Training Tokenizers vs. Training Models\n",
    "\n",
    "It's important to note that training tokenizers differs from training models. Training a model is a stochastic (non-deterministic) process, while training a tokenizer is deterministic and statistical. The tokenizer learns which subwords to use based on the dataset, a design decision of the tokenization algorithm.\n",
    "\n",
    "Popular subword tokenization approaches include Byte-level BPE (used in GPT-2), WordPiece, and SentencePiece. Each method has its advantages and is chosen based on the specific needs of the model and dataset.\n",
    "\n",
    "By understanding tokenization, we can better appreciate how models process text and generate meaningful outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e020faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cb27c55",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
